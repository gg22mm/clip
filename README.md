# clip
改造clip让它支持中文，并支持8K的输入长度

# 已知问题

OpenAI 的 CLIP 也存在三大短板：

1. 文本输入容量非常有限。最多仅支持 77 个 token 的输入，根据 LongCLIP 的实验，实际上其有效输入不超过 20 个 token。

2. 在纯文本检索中表现不佳。主要原因有两点：首先，CLIP 模型的训练目标是对齐文本和图像，没有针对纯文本检索进行专门优化。其次，CLIP 模型的训练数据主要由相对较短的文本组成，难以泛化到更广阔的文本检索场景。

3. 不支持中文

所以想用clip来做向量搜索： 文本-文本、文本-图像、图像-文本、图像-图像四个方向的检索 或 视频 搜索时困难重重。

为什么要用clip来做搜索？ 我们都知道传统搜索都是基于给图片或文字添加 “关键字” 来做搜索的，结果往往非常依赖关键字的弊端，所以才考虑clip
